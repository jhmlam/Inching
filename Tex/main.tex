\documentclass[a4paper]{article}

\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }



\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath}
\usepackage{amsfonts}


\title{Scalable anisotropic vibrations of megascale macromolecules \protect\\ Algorithms \protect\\}



\author{Jordy Homing Lam}

\date{December 27, 2023}

\begin{document}
\maketitle

This is a supporting document submitted to support our article "Scalable anisotropic vibrations of megascale macromolecules". It contains the pseudocodes for the following algorithms:
\begin{itemize}
\item [Algorithm 1] 3-D Reverse Cuthill McKee (3DRCM)
\item [Algorithm 2] Modified Gram-Schmidt Vector (MGSV)
\item [Algorithm 3] Iterative Classical Gram-Schmidt (ICGS)
\item [Algorithm 4] p-step Lanczos Factorization (PLF)
\item [Algorithm 5] Implicitly Restarted Lanczos Method (IRLM)
\item [Algorithm 6] Thick Restart Lanczos Method (TRLM)
\item [Algorithm 7] Jacobi-Davidson Method (JDM)
\item [Algorithm 8] Chebyshev-Davidson Method (CDM)
\item [Algorithm 9] Chebyshev Filtered Matrix Vector Product (ChebAv)
\end{itemize}











\pagebreak







\section{3-D Reverse Cuthill McKee}

A Reverse Cuthill McKee algorithm supplemented with a k-D tree data structure where k=3. Analysis of a k-D tree can be found in the work of `Bentley, J. L. Multidimensional binary search trees used for associative searching. Commun. ACM 18, 509â€“517 (1975)`. An empty 1-d array of length $l$ is notated as $0^l$. Time complexity marked in comment.

\begin{algorithm}
\caption{3-D Reverse Cuthill McKee}\label{alg:3DRCM}
\begin{algorithmic}[1]
\Procedure{3DRCM}{$X \in \mathbb{R}^{N \times 3}$,  $R_C \in \mathbb{R}^{1} $}
\\\Comment{ (a) Prepare a 3-D tree $O(N log N)$}
\State $3dTree \gets KdTree(X)$ 


\\\Comment{(b) Find peripheral node $O(N \times 3N^{2/3})$}
\State Initialize $D = 0^{N}$\Comment{Degrees.}
\For {$i = 0,...,N-1$}
    \State $|\mathcal{N}(X_i)| = 3dTree(R_C, X_i)$
    \State $D[i] \gets |\mathcal{N}(X_i)|$
\EndFor
  
\State $Inds = Argsort(D)$ ; $IndsRev = Argsort(Inds)$ \Comment{Quicksort.}
\State $m = max(D)$


\\\Comment{(c) Breadth-first loop $O(3N^{5/3} + 2cm^{2}N)$}
\State Initialize $L = 0^{N}$ ; $\tilde{D} = 0^{m}; n = 0$

\For {$z=0,...,N-1$}
    \If{$Inds[z] \mathrel{=}= -1$}
    \State continue
    \EndIf

    \State $L[n] \gets Inds[z]$; $n \mathrel{+}= 1$ 

    \State $Inds[IndsRev[Inds[z]]] \gets -1$ \Comment{Indicate visited.}
    \State $Level_{s} = n - 1 ; Level_{t} = n$ 
    
    \While{ $Level_{s} < Level_{t}$}
        \For{$\tilde{i} = Level_{s},..., Level_{t}-1 $}
            \State $i \gets L[\tilde{i}]$ ; $n_{old} = n$
 
            \State $\mathcal{N}(X_i) = 3dTree(R_C, X_i)$ \Comment{ Neighbors in $R_C$.}

            \For{$j = 0,...,|\mathcal{N}(X_i)| -1$}
                \State $\tilde{j} = \mathcal{N}(X_i)[j]$
                \If{$Inds[IndsRev[\tilde{j}]] \mathrel{=}= -1$}
                    \State continue
                \EndIf
                \State $Inds[IndsRev[\tilde{j}]] \gets -1 $ 
                \State $L[n] \gets j$ ; $n \mathrel{+}= 1$
            
            \EndFor
            
            \State $l_s = 0$
            \For{$k = n_{old},...,n-1$}\Comment{Insertion Sort.}
                \State $\tilde{D}[l_s] \gets D[L[k]]$ ; $l_s \mathrel{+}= 1$
            \EndFor

            \For{$k = 1, ...,l_{s} -1$}
                \State $\tilde{d} \gets \tilde{D}[k]$
                \State $\tilde{l} \gets L[n_{old} + k]$
                \State $l_t \gets k$
                \While{ $l_t > 0$ and $\tilde{d} < \tilde{D}[l_t - 1] $}
                    \State $\tilde{D}[l_t] \gets \tilde{D}[l_t - 1]$
                    \State $L[n_{old} +l_t] \gets L[n_{old} + l_t -1]$; $l_t \mathrel{-}= 1$
                \EndWhile
                \State $\tilde{D}[l_t] \gets \tilde{d}$
                \State $L[n_{old} + l_t] \gets \tilde{l} $
            \EndFor
        \EndFor
    \State $Level_s \gets Level_t$ ; $Level_t \gets n$
        
    \EndWhile
    
\EndFor



\\\Comment{ (d) Reverse}
\State \textbf{return} $L[::-1]$


\EndProcedure
\end{algorithmic}
\end{algorithm}




























\pagebreak

\section{Modified Gram-Schmidt Vector}
A Modified Gram-Schmidt (MGS) algorithm to orthogonalize a vector $u$ against the basis $V = [v_0,...,v_m] $. 




\begin{algorithm}
\caption{Modified Gram-Schmidt Vector}\label{alg:MGSV}
\begin{algorithmic}[1]
\Procedure{MGSV}{$u \in \mathbb{R}^{n} ; V \in \mathbb{R}^{m \times n} $}

\For{$i = 0, ..., m-1$}
    \State $ u \gets u - (u^\top v_i) v_i$
\EndFor
\State \textbf{return} $ u $
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{Iterative Classical Gram-Schmidt}
An Iterative Classical Gram-Schmidt (ICGS) algorithm to orthogonalize a vector $u$ against the basis $V = [v_0,...,v_m] $. A break is triggered when the deflation is revoked.




\begin{algorithm}
\caption{Iterative Classical Gram-Schmidt}\label{alg:MGSV}
\begin{algorithmic}[1]
\Procedure{ICGS}{$u \in \mathbb{R}^{n} ; V \in \mathbb{R}^{m \times n} $}
\State $r_0 = ||u||_2$
\For{$i_{iter} = 1,2,3$}
    \State $ u \gets u - VV^\top u$
    \State $ r_1 = ||u||_2$
    \If{$r_1 > r_0 / 2 $}
        \State Break
    \EndIf
    \State $r_0 \gets r_1$
\EndFor
\If{$r_1 \le r_0/2$}
\State Warning! Loss of orthogonality
\EndIf
\State \textbf{return} $ u $
\EndProcedure
\end{algorithmic}
\end{algorithm}

\section{p-step Lanczos Factorization}
A p-step Lanczos Factorization (PLF) to produce $p$ basis between index $j_s$ and $j_t$. Algorithm 4 PLF is where a polynomial filter can be applied, for example, in Algorithm 8 CF with a first-kind Chebyshev polynomial. Step 9 and 10 are optional; a proof of the spectrum bound can be found in `Zhou et al, Bounding the spectrum of large Hermitian matrices,
Linear Algebra and its Applications 435(3), 2011, p.480-493, https://doi.org/10.1016/j.laa.2010.06.034`. In our case, the lower bound is well known.

\begin{algorithm}
\caption{p-step Lanczos Factorization}\label{alg:PLF}
\begin{algorithmic}[1]
\Procedure{PLF}{$A$ $\in \mathbb{R}^{n \times n} , V \in \mathbb{R}^{(m+1)\times n},  \alpha \in \mathbb{R}^{m},  \beta \in \mathbb{R}^{m}, j_s = 0, j_t = m $ }


\For{$j = j_s, ..., j_t - 1$}
    \State $r = Av_j$ \Comment{$r = p(A, v_j, a, b)$ if filter in use.}
    \State $\alpha_j \gets v_j^\top r  $
    \State $r \gets r - \alpha_j v_j $
    \State $r \gets MGSV(r,V[:j]); r \gets MGSV(r,V[:j])$ \Comment{Full Reorthogonalization (FRO).}
    \State $\beta_j \gets ||r||_2  $
    \State $v_{j+1} = r / \beta_j$
\EndFor
\State $T[j,j] = \alpha; T[j,j-1] = T[j-1,j] = \beta$
\State Solve $TQ=QD$ \Comment{Spectrum upper bound as $D_{max} + \beta_j ||e_j^\top Q_{j}||_\infty$}


\State \textbf{return} $V, \alpha, \beta$ 

\EndProcedure
\end{algorithmic}
\end{algorithm}



\pagebreak





\section{Implicitly Restarted Lanczos Method}
An Implicitly Restarted Lanczos Method (IRLM) to compute $k$ smallest eigenpairs. This algorithm calls Algorithm 2 MGSV and Algorithm 4 PLF. At max, 15000 restarts were allowed.


\begin{algorithm}
\caption{Implicitly Restarted Lanczos Method}\label{alg:IRLM}
\begin{algorithmic}[1]
\Procedure{IRLM}{$A$ $\in \mathbb{R}^{n \times n} , V \in \mathbb{R}^{(m+1)\times n},  \alpha \in \mathbb{R}^{m},  \beta \in \mathbb{R}^{m} , \epsilon=1e-8$ }



\State Initialize $v_0 \gets v_{rand}/||v_{rand}||_2 $
\State $j_s = 0$
\For{$i_{iter} = 1,...,15000$}
        \\\Comment{(a) Initial p-steps Lanczos Factorization}

        \State PLF(A,V,$\alpha$,$\beta$, $j_s = j_s$, $j_t = m$) 



        \\\Comment{(b) Implicit Shift}
        \State $ S,W = eig(\alpha, \beta[:m-1])$
        \State $ \theta = Sort(Diag(S))[::-1][:(m-k)]$ 
        \State $\tilde{\beta} = \beta_{k+p-1}$
        \State $Q = I$
        \For{$i = 0...p-1$}
            \State $T = Tridiag(\alpha, \beta[:m-1]) $
            \State $\tilde{Q} \tilde{R} = T -  \theta_i I$
            \State $T \gets \tilde{Q}^\top T \tilde{Q}$
            \State $ \alpha, \beta \gets T$ 
            \State $ Q \gets Q\tilde{Q}$
        \EndFor
        \State $\beta \gets [\beta , \tilde{\beta}]$ 
        \\\Comment{(c) Implicit Restart}
        \State $\sigma = Q[k-1,m-1]$
        \State $V[:k] \gets Q[:k,:] V[:m,:]$
        \State $v_k \gets \beta_{k-1}  Q[k,:] V[:m,:] + \sigma \beta_{m-1}  v_{m}$
        \State $\beta_{k-1} \gets ||v_k||_2 $
        \State $v_k \gets v_k / \beta_{k-1}$

        \State $v_{k} \gets MGSV(v_{k}, V[:k]); v_{k} \gets MGSV(v_{k}, V[:k])$

        \State $v_{k} \gets v_{k} / ||v_{k}||_2$
       
        \State $j_s \gets k $ \Comment{Reset the PLF}
        \\\Comment {(d) Estimate Convergence}
        \If {$|\beta_{k-1}| < \epsilon$}
            \State Break
        \EndIf
            
\EndFor

\State $S,W = eig(Tridiag(\alpha[:k], \beta[:k-1]))$


\State \textbf{return} $S[:k], W^\top V[:k,:]$

\EndProcedure
\end{algorithmic}
\end{algorithm}





















\pagebreak

\section{Thick Restart Lanczos Method}
A Thick Restart Lanczos Method to compute the $k$ smallest eigenpairs. This algorithm calls Algorithm 2 MGSV and Algorithm 4 PLF. Algorithm 4 PLF is where a  filter can be applied through the matrix-vector multiplication, for example, in Algorithm 9, a filter is constructed using first-kind Chebyshev polynomial basis. At max, 15000 restarts were allowed.


\begin{algorithm}
\caption{Thick Restart Lanczos Method}\label{alg:TRLM}
\begin{algorithmic}[1]
\Procedure{TRLM}{$A$ $\in \mathbb{R}^{n \times n} , V \in \mathbb{R}^{(m+1)\times n},  \alpha \in \mathbb{R}^{m},  \beta \in \mathbb{R}^{m} , \epsilon=1e-12$ }
\State Initialize $v_0 \gets v_{rand}/||v_{rand}||_2 $
\State $j_s = 0; b = 0; S = 0; r = v_0$
\For{$i_{iter} = 1,...,15000$}

        \\\Comment{(a) Reinitialize}
        \State $\beta[:k] \gets 0$
        \State $\alpha[:k] \gets Diag(S)$
        \State $r \gets MGSV(r,V[:k])$
        \State $r \gets r / ||r||_2$
        \State $v_k \gets r$

        \\\Comment{(b) p-step Lanczos Factorization, filter inside.}
        
        \State PLF(A,V,$\alpha$,$\beta$, $j_s = j_s$, $j_t = m$) 
        \\\Comment{(c) Thick Restart}
        \State $T = Tridiag(\alpha,\beta[:m-1]); T[k,:k] \gets b; T[:k,k] \gets b$
        \State $S,W = eig(T)$
        \State $W \gets W[:m,Argort(Diag(S))[:k]]$
        \State $S \gets Diag(S)[Argort(Diag(S))[:k]]$
        \State $V[:k,:] \gets W^\top V$
        \State $b = \beta_{m-1} W[m-1, :k]$ 
        
        \State $j_s \gets k$ \Comment{Reset the PLF}
        \\\Comment{(d) Check Convergence}
        \If{$||b||_2 < \epsilon$}
            \State Break
        \EndIf
        
        
\EndFor



\State \textbf{return} $S, V$

\EndProcedure
\end{algorithmic}
\end{algorithm}


























\pagebreak
\section{Jacobi-Davidson Method}

A Jacobi-Davidson Method to compute $k$ smallest eigenpairs. This algorithm calls Algorithm 2 MGSV and Algorithm 3 ICGS. The correction $z$ is approximated with Generalized Minimal Residual Iteration (GMRES) routine provided in CuPy; unless otherwise stated, the iteration is stopped if residual error in GMRES reach 1e-6 or the number of iterations exceeds 20. Note that this can be replaced by a Minimal Residual Iteration (MIRES) routine, which processes symmetric matrices. $j_s$ counts towards the desired number of converged Ritz pair $k$. Some preconditioners (e.g. ILU(k)) were considered in solving the correction equation, but their size and density become prohibitive as $n$ increases; trivial preconditioner e.g. $diag(A)^{-1}$ does not improve performance.

\begin{algorithm}
\caption{Jacobi-Davidson Method}\label{alg:JDM}
\begin{algorithmic}[1]
\Procedure{JDM}{$A$ $\in \mathbb{R}^{n \times n} , \epsilon=1e-12,  k=64$ }

\State Initialize $v_0 = v_{rand} / ||v_{rand}||_2$ 
\State $G = v_0^\top A v_0$ 
\State $V = [v_0,]$ 
\State $Q = [0^n,]; \Lambda = [] $

\State $j_s = 0$ 
\\

\For{$i_{iter} = 1,...,15000$}

    \State $ S,W = eig(G)$
    \While{True}
        \\\Comment{(a) Get Residual}
        \State $u = VW[:,0]$; $\theta = S[0,0]$
        \State $r = Au - \theta u$
        \State $\sigma = \theta $\Comment{Propose shift.}
        
        \State $\tilde{Q} = [Q,u]$
    \\
        \If{$(||r||_2 > \epsilon) \cup ( (dim(S)[0] <= 1) \cap (j_s \mathrel{!}= k - 1))$]} \Comment{Non-convergence.}
            \State Break.
        \EndIf
        \\\Comment{(b) Update Projections}
        \State $\Lambda \gets [\Lambda, \theta]$
        \State $Q \gets \tilde{Q}$
        \State $V \gets V W[:,1:j]; S \gets S[1:j,1:j]$
        \State $G \gets S ; W \gets I$
        \State $j_s \mathrel{+} = 1$

        \If{$j_s == k$}
            \State Return $\Lambda, Q$ \Comment{All converged.}
        \EndIf

    \EndWhile
    \\ 
    \If{$dim(S)[0] == 2k$}\Comment{(c) Restart if workspace is full}
        \State $V \gets VW[:, 0:k] ; S \gets S[0:k,0:k] $
        \State $G \gets S; W \gets I $
    \EndIf

    %\If{$||r||_2 < \epsilon$}\Comment{Override shift.}
    %    \State $\sigma = \theta$
    %\EndIf
    \\\Comment{(d) Correction equation with GMRES.}
    \State $(I-uu^\top) (A - \theta I ) (I -uu^\top) z = -r$ 
    \State $z \gets MGSV(\tilde{Q}, z)$ 
    \State $z \gets ICGS(V, z)$ ; $z \gets ICGS(V, z)$
    \State $z \gets z / ||z||_2$
    \\\Comment{(e) Update Projections}
    \State $\tilde{z} = Az$
    \State $V \gets [V,z]$ 
    \State $G \gets [G, V^\top \tilde{z} ; \tilde{z}^\top V, z^\top \tilde{z}]$ 

\EndFor

\\\Comment{(f) Guard not all converged.}
\State \textbf{return} $\Lambda, Q$ 
\EndProcedure
\end{algorithmic}
\end{algorithm}



\pagebreak

\section{Chebyshev-Davidson Method}
A Chebyshev-Davidson Method to compute $k$ smallest eigenpairs. This algorithm calls Algorithm 2 MGSV and Algorithm 3 ICGS. $\lambda_{min}$ is the lower bound of spectrum. $\lambda_{max}$ is upper bound of the spectrum obtained from Algorithm 4 PLF. $\theta_s$ is the 'squeezing' moving lower bound.




\begin{algorithm}
\caption{Chebyshev-Davidson Method}\label{alg:CDM}
\begin{algorithmic}[1]
\Procedure{CDM}{$A$ $\in \mathbb{R}^{n \times n} , \lambda_{min}, \lambda_{max}, \epsilon=1e-12, k=64, $}




\State Initialize $v_0 = v_{rand} / ||v_{rand}||_2$ 
\State $G = v_0^\top A v_0$ 
\State $V = [v_0,]$ 
\State $Q = [Av_0,]; \Lambda = [] $
\State $j_s = 0 ;j = 1$ 
\State $u = V[:,j_s]$
%\State $\epsilon_r = \epsilon max(1e-14, |G[0,0]|)$
\State $\theta_s = (\lambda_{max} + G[0,0])/2$




\\

\For{$i_{iter} = 1,...,15000$}


    \State $z = p(A, u, \theta_s, \lambda_{max}) $ \Comment{(a) Low-pass Chebyshev filter}

    \State $z \gets MGSV(\tilde{Q}, z)$ 
    \State $z \gets ICGS(V, z)$ 
    \State $z \gets z / ||z||_2$




    \\\Comment{(b) Update Projections}
    \State $\tilde{z} = Az$
    \State $V[:,j] \gets z$
    \State $Q[:,j] \gets \tilde{z}$

    \State $G[j, j_s:j] \gets Q[:,j]^\top V[:,j_s:j+1]$
    \State $G[j_s:j, j] \gets G[j, j_s:j]^\top $
    \State $ S,W = eig(G)$

    \\\Comment{(c) Restart if workspace is full}
    \State $j_r = j + 1$
    \If{$j+1 >= 2k$}
        \State $j_r = max(j_s+1, k+5, min(k + j_s, 2k -5))$
    \EndIf
    
    \State $V[:,j_s:j_r] \gets V[:,j_s:j+1]W[:, 0:j_r - j_s + 1] $
    \State $Q[:,j_s:j_r] \gets Q[:,j_s:j+1]W[:, 0:j_r - j_s + 1] $
    \State $G[j_s:j_r,j_s:j_r] \gets S[0:j_r - j_s + 1,0:j_r - j_s + 1]; W \gets I $

    \\\Comment{(d) Get Residual}
    \State $\theta = S[0]$
    \State $r = Q[:, j_s] - \theta V[:,j_s]$
    \State $u = V[:, j_s]$ \Comment{Next $u$}
    
    \If {$||r||_2 < \epsilon$}
        \State $\Lambda \gets [\Lambda, \theta]; j_s += 1$
        \If {$j_s >= k$}
            \State  Return $\Lambda, Q$ \Comment{All converged.}
        \EndIf
        \State $u = V[:, j_s -1]$ \Comment{Next $u$}
    \EndIf
    \State $j = j_r$
    
    \\\Comment{(e) Update moving lower bound}

    \State $\theta_s = max(S_{median}, \lambda_{min})$



\EndFor

\\\Comment{(f) Guard not all converged.}
\State \textbf{return} $\Lambda, Q$ 
\EndProcedure

\end{algorithmic}
\end{algorithm}


\pagebreak


\section{Chebyshev Filtered Matrix Vector Product}
Matrix vector product filtered on the basis of first-kind Chebyshev polynomial. $p(t) =  \sum_{j=0}^{j=M}{\kappa_j T_j(t)}$. Note that (1) $\kappa_j$ is a set of user-defined coefficients e.g. the scaled coefficients in equation 28 of the main text. (2) In Algorithm 8 CDM, only the M-th degree polynomial is considered.  

\begin{algorithm}
\caption{Chebyshev Filtered Matrix Vector Product}\label{alg:ChebAv}
\begin{algorithmic}[1]
\Procedure{ChebAv}{$A$ $\in \mathbb{R}^{n \times n} , v \in \mathbb{R}^{n\times 1} , a ,b$ }

\State $e =:(b-a)/2 ; c=: (b+a)/2$
\State $y =  \kappa_0 v$
\State $v_{+} =  \kappa_1 (Av-cv)/e$
\State $y \mathrel{+}=  \kappa_1 v_{+}$
\State $v_{-} = v; v \gets v_{+}; $
\For{$j = 2, ..., M$}
    \State $v_{+} =  2/e  (Av - cv) - v_{-}$
    \State $y \mathrel{+}= \kappa_j v_{+}$
    \State $v_{-} \gets v; v \gets v_{+} $

\EndFor



\State \textbf{return} $y$ 

\EndProcedure
\end{algorithmic}
\end{algorithm}



\end{document}
